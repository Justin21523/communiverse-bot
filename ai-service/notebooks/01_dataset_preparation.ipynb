{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865cf54c",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Finetuning,\n",
    "This notebook handles:,\n",
    "- Loading raw data\\,\n",
    "- Formatting for different tasks (SFT, DPO, Persona, Story)\\,\n",
    "- Train/validation splitting,\n",
    "- Tokenization and saving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\\n\",\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"  # or Qwen, Llama, etc.\n",
    "RAW_DATA_PATH = \"../data/raw/conversations.jsonl\"\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "MAX_LENGTH = 2048\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72120f84",
   "metadata": {},
   "source": [
    "## Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data = []\n",
    "with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd54374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\\n\",\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Average prompt length: {df['prompt'].str.len().mean():.0f} chars\")\n",
    "print(f\"Average response length: {df['response'].str.len().mean():.0f} chars\")\n",
    "\n",
    "# Distribution of lengths\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['prompt'].str.len(), bins=50)\n",
    "plt.xlabel('Prompt Length (chars)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prompt Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['response'].str.len(), bins=50)\n",
    "plt.xlabel('Response Length (chars)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Response Length Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd31ef3",
   "metadata": {},
   "source": [
    "## Format Data for SFT (Supervised Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_sft(examples):\n",
    "    \"\"\"\n",
    "    Format data for supervised fine-tuning\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        # Chat template format\\n\",\n",
    "        text = f\"\"\"<|im_start|>system\n",
    "You are a helpful AI assistant for the Elio/Pixar community.<|im_end|>\n",
    "<|im_start|>user\n",
    "{examples['prompt'][i]}<|im_end|>\n",
    "<|im_start|>assistant\\n\",\n",
    "{examples['response'][i]}<|im_end|>\"\"\"\n",
    "        formatted.append(text)\n",
    "\n",
    "    return {'text': formatted}\n",
    "\n",
    "# Apply formatting\n",
    "df_sft = df.copy()\n",
    "formatted_data = format_for_sft({\n",
    "    'prompt': df_sft['prompt'].tolist(),\n",
    "    'response': df_sft['response'].tolist()\n",
    "})\n",
    "\n",
    "df_sft['text'] = formatted_data['text']\n",
    "print(f\"\\nFormatted {len(df_sft)} samples for SFT\")\n",
    "print(\"\\nExample:\")\n",
    "print(df_sft['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee249d",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\\n\",\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "# Create HuggingFace Dataset\\n\",\n",
    "dataset = Dataset.from_pandas(df_sft[['text']])\n",
    "\n",
    "# Tokenize\\n\",\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06177fc",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\\n\",\n",
    "split_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e049a8",
   "metadata": {},
   "source": [
    "## Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50076c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatasetDict\\n\",\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    ",\n",
    "# Save to disk\\n\",\n",
    "output_path = os.path.join(OUTPUT_DIR, 'sft_dataset')\n",
    "dataset_dict.save_to_disk(output_path)\n",
    "print(f\"\\nDataset saved to: {output_path}\")\n",
    "\n",
    "# Also save metadata\\n\",\n",
    "metadata = {\n",
    "    'base_model': BASE_MODEL,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'train_samples': len(train_dataset),\n",
    "    'val_samples': len(val_dataset),\n",
    "    'validation_split': VALIDATION_SPLIT,\n",
    "    'format': 'sft',\n",
    "    'created_at': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'dataset_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba39bfa",
   "metadata": {},
   "source": [
    "## Token Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af538380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token lengths\\n\",\n",
    "token_lengths = [len(x['input_ids']) for x in train_dataset]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.axvline(MAX_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_LENGTH})')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Token Lengths in Training Set')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nToken Length Statistics:\")\n",
    "print(f\"Mean: {pd.Series(token_lengths).mean():.0f}\")\n",
    "print(f\"Median: {pd.Series(token_lengths).median():.0f}\")\n",
    "print(f\"95th percentile: {pd.Series(token_lengths).quantile(0.95):.0f}\")\n",
    "print(f\"Max: {pd.Series(token_lengths).max():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
