{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0d94bb",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with LoRA/QLoRA\n",
    "\n",
    "This notebook covers:\n",
    "- Loading base model and dataset\n",
    "- Configuring LoRA/QLoRA for efficient training\n",
    "- Training with Trainer API\n",
    "- Evaluation and metrics\n",
    "- Saving and merging adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\\n\",\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a48e98",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead62924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and dataset configuration\\n\",\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"  # or \\\"Qwen/Qwen2.5-14B\\\", \\\"meta-llama/Llama-3.1-8B\\\"\\n\"\n",
    "DATASET_PATH = \"../data/processed/sft_dataset\"\n",
    "OUTPUT_DIR = \"../models/sft_lora\"\n",
    "\n",
    "# LoRA configuration\\n\",\n",
    "LORA_R = 16  # Rank\\n\",\n",
    "LORA_ALPHA = 32  # Scaling factor (typically 2x rank)\\n\",\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\",\n",
    "                       \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training hyperparameters\\n\",\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Optimization settings\\n\",\n",
    "USE_8BIT = False  # Set True for QLoRA (8-bit quantization)\\n\",\n",
    "USE_4BIT = True   # Set True for 4-bit quantization\\n\",\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "USE_FP16 = True if not USE_4BIT else False\n",
    "USE_BF16 = False  # Use BF16 if your GPU supports it (A100, H100)\\n\",\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80ae65",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dataset\\n\",\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nDataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b43c9e",
   "metadata": {},
   "source": [
    "## Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\\n\",\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization if enabled\\n\",\n",
    "model_kwargs = {\"device_map\": \"auto\"}\n",
    "\n",
    "if USE_4BIT:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    model_kwargs[\"quantization_config\"] = bnb_config # type: ignore\n",
    "    print(\"Using 4-bit quantization (QLoRA)\")\n",
    "\n",
    "elif USE_8BIT:\n",
    "    model_kwargs[\"load_in_8bit\"] = True  # type: ignore\n",
    "    print(\"Using 8-bit quantization\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    **model_kwargs,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training if using quantization\\n\",\n",
    "if USE_4BIT or USE_8BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"\\nModel loaded: {BASE_MODEL}\")\n",
    "print(f\"Model parameters: {model.num_parameters() / 1e9:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506157a",
   "metadata": {},
   "source": [
    "## Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\\n\",\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\\n\",\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\\n\",\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(\"Rank: {LORA_R}\")\n",
    "print(\"Alpha: {LORA_ALPHA}\")\n",
    "print(\"Dropout: {LORA_DROPOUT}\")\n",
    "print(\"Target modules: {LORA_TARGET_MODULES}\")\n",
    "print(\"\\nTrainable parameters: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "print(\"Total parameters: {total_params:,}\")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9abc8",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\\n\",\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    # Optimization\n",
    "    fp16=USE_FP16,\n",
    "    bf16=USE_BF16,\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    optim=\"paged_adamw_8bit\" if USE_4BIT else \"adamw_torch\",\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # Evaluation\\n\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # Other\n",
    "    report_to=\"none\",  # Change to \"wandb\" if using Weights & Biases\n",
    "    remove_unused_columns=False,\n",
    "    ddp_find_unused_parameters=False if USE_GRADIENT_CHECKPOINTING else None,\n",
    ")\n",
    "\n",
    "print(f\"\\nEffective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e75fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b347e03",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654debd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92483fdf",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\\n\",\n",
    "print(\"Starting training...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTraining time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd8991",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccede61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9948d",
   "metadata": {},
   "source": [
    "## Save Model and Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\\n\",\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nLoRA adapters saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save training metrics\\n\",\n",
    "import json\n",
    "metrics = {\n",
    "    \"train_loss\": train_result.metrics['train_loss'],\n",
    "    \"train_runtime\": train_result.metrics['train_runtime'],\n",
    "    \"eval_loss\": eval_results['eval_loss'],\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Training metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc97ebd",
   "metadata": {},
   "source": [
    "## Merge and Save Full Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters with base model for inference\n",
    "MERGE_AND_SAVE = False  # Set True to merge and save full model\n",
    "\n",
    "if MERGE_AND_SAVE:\n",
    "    print(\"Merging LoRA adapters with base model...\")\n",
    "\n",
    "    # Merge adapters\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Save merged model\n",
    "    merged_output_dir = OUTPUT_DIR + \"_merged\"\n",
    "    model.save_pretrained(merged_output_dir)\n",
    "    tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "    print(f\"Merged model saved to: {merged_output_dir}\")\n",
    "else:\n",
    "    print(\"Skipping model merging (use LoRA adapters for inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a904c",
   "metadata": {},
   "source": [
    "## Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\\n\",\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test prompts\\n\",\n",
    "test_prompts = [\n",
    "    \"Tell me about Elio's first day at the Communiverse.\",\n",
    "    \"What makes Glordon such a mysterious character?\",\n",
    "    \"Describe the relationship between Elio and Ambassador Questa.\"\n",
    "]\n",
    "\n",
    "print(\"\\nInference Tests:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: \", end=\"\")\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=200,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    print(result[0]['generated_text'][len(prompt):])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
