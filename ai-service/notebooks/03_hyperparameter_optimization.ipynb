{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed7b7f7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Hyperparameter Optimization with Optuna\n",
    "# \n",
    "# This notebook uses Optuna for automated hyperparameter tuning:\n",
    "# - Learning rate optimization\n",
    "# - Batch size and gradient accumulation\n",
    "# - LoRA rank and alpha\n",
    "# - Warmup and weight decay\n",
    "# - Early stopping and scheduling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152db338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import libraries and configuration\n",
    "import os\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"\n",
    "DATASET_PATH = \"../data/processed/sft_dataset\"\n",
    "OUTPUT_DIR = \"../models/optuna_trials\"\n",
    "N_TRIALS = 20\n",
    "TIMEOUT = 3600 * 6  # 6 hours\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa3f60",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdd87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load and prepare dataset\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Use smaller subset for faster tuning\n",
    "train_subset = train_dataset.select(range(min(5000, len(train_dataset))))\n",
    "val_subset = val_dataset.select(range(min(500, len(val_dataset))))\n",
    "\n",
    "print(f\"Training subset: {len(train_subset)} samples\")\n",
    "print(f\"Validation subset: {len(val_subset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c420d10",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Define Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna optimization\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [2, 4, 8])\n",
    "    gradient_accumulation_steps = trial.suggest_categorical(\n",
    "        \"gradient_accumulation_steps\", [2, 4, 8]\n",
    "    )\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.1)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "\n",
    "    # LoRA hyperparameters\n",
    "    lora_r = trial.suggest_categorical(\"lora_r\", [8, 16, 32, 64])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64])\n",
    "    lora_dropout = trial.suggest_float(\"lora_dropout\", 0.0, 0.2)\n",
    "\n",
    "    # Scheduler\n",
    "    lr_scheduler_type = trial.suggest_categorical(\n",
    "        \"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\"]\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model with 4-bit quantization\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/trial_{trial.number}\",\n",
    "        num_train_epochs=1,  # Short epoch for fast tuning\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=50,\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=val_subset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_loss = eval_results[\"eval_loss\"]\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4a95e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1584308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"sft_hyperparameter_tuning\",\n",
    "    storage=f\"sqlite:///{OUTPUT_DIR}/optuna_study.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(f\"Starting optimization with {N_TRIALS} trials...\\n\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Optimization completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558064a3",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Display best trial results\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"Best Validation Loss: {best_trial.value:.4f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b147828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Visualize optimization results\n",
    "# Optimization history\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "# Parameter importances\n",
    "fig = plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save best hyperparameters\n",
    "import json\n",
    "\n",
    "best_params = {\n",
    "    \"best_trial\": best_trial.number,\n",
    "    \"best_loss\": best_trial.value,\n",
    "    \"params\": best_trial.params,\n",
    "    \"n_trials\": len(study.trials),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/best_hyperparameters.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to: {OUTPUT_DIR}/best_hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fbf12",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Trial Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6261fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create and display trials DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df = trials_df.sort_values(\"value\")\n",
    "\n",
    "print(\"\\nTop 5 Trials:\")\n",
    "print(trials_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "trials_df.to_csv(f\"{OUTPUT_DIR}/trials_results.csv\", index=False)\n",
    "print(f\"\\nAll trials saved to: {OUTPUT_DIR}/trials_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
