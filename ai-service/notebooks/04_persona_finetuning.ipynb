{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d2303c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Persona-Specific Finetuning\n",
    "# \n",
    "# Train persona-specific adapters for characters like Elio, Glordon, etc.\n",
    "# - Load persona conversation data\n",
    "# - Format with persona-specific prompts\n",
    "# - Train separate LoRA adapters per character\n",
    "# - Test persona consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355249e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import libraries and configuration\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"\n",
    "PERSONAS_DATA_DIR = \"../data/personas\"\n",
    "OUTPUT_DIR = \"../models/persona_adapters\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd909c57",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Define Persona Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Persona configurations\n",
    "PERSONA_CONFIGS = {\n",
    "    \"Elio\": {\n",
    "        \"traits\": [\"curious\", \"optimistic\", \"slightly awkward\", \"enthusiastic\"],\n",
    "        \"tone\": \"friendly and genuine\",\n",
    "        \"style\": \"casual with occasional excitement\",\n",
    "        \"background\": \"Earth kid accidentally invited to join the Communiverse\",\n",
    "    },\n",
    "    \"Glordon\": {\n",
    "        \"traits\": [\"wise\", \"mysterious\", \"patient\", \"cryptic\"],\n",
    "        \"tone\": \"sage-like and thoughtful\",\n",
    "        \"style\": \"speaks in riddles and metaphors\",\n",
    "        \"background\": \"Ancient alien guardian of cosmic knowledge\",\n",
    "    },\n",
    "    \"Ambassador Questa\": {\n",
    "        \"traits\": [\"professional\", \"diplomatic\", \"proper\", \"formal\"],\n",
    "        \"tone\": \"official and courteous\",\n",
    "        \"style\": \"precise language with diplomatic phrasing\",\n",
    "        \"background\": \"Communiverse diplomatic representative\",\n",
    "    },\n",
    "    \"Lord Grigon\": {\n",
    "        \"traits\": [\"gruff\", \"short-tempered\", \"secretly caring\", \"traditional\"],\n",
    "        \"tone\": \"brusque but not unkind\",\n",
    "        \"style\": \"terse sentences with occasional warmth\",\n",
    "        \"background\": \"Stern but fair Communiverse leader\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Configured personas:\")\n",
    "for name, config in PERSONA_CONFIGS.items():\n",
    "    print(f\"- {name}: {', '.join(config['traits'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f73197",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Load and Format Persona Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4354540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def format_persona_data(persona_name, conversations):\n",
    "    \"\"\"\n",
    "    Format conversations for persona-specific training\n",
    "    \"\"\"\n",
    "    config = PERSONA_CONFIGS[persona_name]\n",
    "    formatted = []\n",
    "\n",
    "    system_prompt = f\"\"\"You are {persona_name}.\n",
    "Personality: {', '.join(config['traits'])}\n",
    "Tone: {config['tone']}\n",
    "Style: {config['style']}\n",
    "Background: {config['background']}\n",
    "\n",
    "Always stay in character and respond naturally as {persona_name} would.\"\"\"\n",
    "\n",
    "    for conv in conversations:\n",
    "        text = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{conv['user_message']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{conv['persona_response']}<|im_end|>\"\"\"\n",
    "        formatted.append({\"text\": text})\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Example: Load persona conversations (mock data)\n",
    "# In production, load from actual conversation logs\n",
    "elio_conversations = [\n",
    "    {\n",
    "        \"user_message\": \"Tell me about your first day in the Communiverse.\",\n",
    "        \"persona_response\": \"Oh wow, it was... honestly, it was overwhelming! I mean, one minute I'm just a regular kid from Earth, and the next I'm surrounded by aliens from all over the galaxy. But everyone was so welcoming, even if I did accidentally press the wrong button on like, five different things.\",\n",
    "    },\n",
    "    # Add more conversations...\n",
    "]\n",
    "\n",
    "formatted_elio_data = format_persona_data(\"Elio\", elio_conversations)\n",
    "print(f\"\\nFormatted {len(formatted_elio_data)} Elio conversations\")\n",
    "print(\"\\nExample:\")\n",
    "print(formatted_elio_data[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abe3d9",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Train Persona Adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b044ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_persona_adapter(persona_name, training_data):\n",
    "    \"\"\"\n",
    "    Train a persona-specific LoRA adapter\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training adapter for: {persona_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(training_data)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], truncation=True, max_length=2048, padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # Split train/val\n",
    "    split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    # Load model\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA config (persona-specific might use different rank)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Training args\n",
    "    output_dir = f\"{OUTPUT_DIR}/{persona_name.lower().replace(' ', '_')}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        warmup_ratio=0.03,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=split[\"train\"],\n",
    "        eval_dataset=split[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Save\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Save persona config\n",
    "    with open(f\"{output_dir}/persona_config.json\", \"w\") as f:\n",
    "        json.dump(PERSONA_CONFIGS[persona_name], f, indent=2)\n",
    "\n",
    "    print(f\"\\nAdapter saved to: {output_dir}\")\n",
    "\n",
    "    # Clean up\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Train Elio adapter\n",
    "elio_adapter_path = train_persona_adapter(\"Elio\", formatted_elio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f9c1e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Test Persona Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def test_persona_consistency(persona_name, adapter_path, test_prompts):\n",
    "    \"\"\"\n",
    "    Test if persona maintains consistent character\n",
    "    \"\"\"\n",
    "    from peft import PeftModel\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Load base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Load adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "    # Create generator\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTesting {persona_name} consistency:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "\n",
    "        result = generator(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        response = result[0][\"generated_text\"][len(prompt) :]\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What do you think about your role in the Communiverse?\",\n",
    "    \"How do you handle pressure?\",\n",
    "    \"Tell me about your friends.\",\n",
    "]\n",
    "\n",
    "test_persona_consistency(\"Elio\", elio_adapter_path, test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5133d63",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Train All Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Train adapters for all personas\n",
    "# (In production, load actual conversation data for each persona)\n",
    "\n",
    "trained_adapters = {}\n",
    "\n",
    "for persona_name in PERSONA_CONFIGS.keys():\n",
    "    # Load persona-specific data (mock for now)\n",
    "    persona_data = []  # Load from files\n",
    "\n",
    "    if len(persona_data) > 0:\n",
    "        formatted_data = format_persona_data(persona_name, persona_data)\n",
    "        adapter_path = train_persona_adapter(persona_name, formatted_data)\n",
    "        trained_adapters[persona_name] = adapter_path\n",
    "\n",
    "        print(f\"\\nâœ“ {persona_name} adapter trained\")\n",
    "\n",
    "print(f\"\\n\\nAll persona adapters trained:\")\n",
    "for name, path in trained_adapters.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
