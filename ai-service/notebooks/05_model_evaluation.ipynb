{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e930d97",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Model Evaluation and Benchmarking\n",
    "# \n",
    "# Comprehensive evaluation:\n",
    "# - Perplexity measurement\n",
    "# - Generation quality metrics\n",
    "# - Persona consistency scoring\n",
    "# - Human evaluation templates\n",
    "# - A/B testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import libraries and configuration\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"deepseek-ai/DeepSeek-V3-Base\"\n",
    "FINETUNED_MODEL = \"../models/sft_lora\"\n",
    "TEST_DATASET = \"../data/processed/sft_dataset\"\n",
    "RESULTS_DIR = \"../evaluation_results\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068a339",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8bdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL)\n",
    "ft_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(ft_base, FINETUNED_MODEL)\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91f744",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Perplexity Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def calculate_perplexity(model, tokenizer, texts, batch_size=8):\n",
    "    \"\"\"\n",
    "    Calculate perplexity on a list of texts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            encodings = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "            ).to(model.device)\n",
    "\n",
    "            outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Calculate number of actual tokens (excluding padding)\n",
    "            n_tokens = (encodings[\"attention_mask\"].sum()).item()\n",
    "\n",
    "            total_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Load test data\n",
    "dataset = load_from_disk(TEST_DATASET)\n",
    "test_texts = dataset[\"validation\"][\"text\"][:100]  # Sample for faster evaluation\n",
    "\n",
    "print(\"Calculating perplexity...\\n\")\n",
    "\n",
    "# Base model perplexity\n",
    "base_ppl = calculate_perplexity(base_model, base_tokenizer, test_texts)\n",
    "print(f\"Base Model Perplexity: {base_ppl:.2f}\")\n",
    "\n",
    "# Fine-tuned model perplexity\n",
    "ft_ppl = calculate_perplexity(ft_model, ft_tokenizer, test_texts)\n",
    "print(f\"Fine-tuned Model Perplexity: {ft_ppl:.2f}\")\n",
    "\n",
    "# Improvement\n",
    "improvement = ((base_ppl - ft_ppl) / base_ppl) * 100\n",
    "print(f\"\\nImprovement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b85579",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Generation Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "def evaluate_generation_quality(model, tokenizer, test_pairs, num_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate generation quality using ROUGE and BLEU\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    smoothing = SmoothingFunction().method1\n",
    "\n",
    "    rouge_scores = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, (prompt, reference) in enumerate(tqdm(test_pairs[:num_samples])):\n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated = generated[len(prompt) :].strip()\n",
    "\n",
    "        # ROUGE\n",
    "        try:\n",
    "            rouge_score = rouge.get_scores(generated, reference)[0]\n",
    "            rouge_scores.append(rouge_score[\"rouge-l\"][\"f\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # BLEU\n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        generated_tokens = nltk.word_tokenize(generated)\n",
    "        bleu = sentence_bleu(\n",
    "            [reference_tokens], generated_tokens, smoothing_function=smoothing\n",
    "        )\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return {\"rouge_l\": np.mean(rouge_scores), \"bleu\": np.mean(bleu_scores)}\n",
    "\n",
    "\n",
    "# Prepare test pairs (prompt, reference)\n",
    "test_pairs = []  # Load from dataset\n",
    "# For demo purposes:\n",
    "test_pairs = [\n",
    "    (\"Tell me about Elio.\", \"Elio is a curious Earth kid who joined the Communiverse.\"),\n",
    "    # Add more...\n",
    "]\n",
    "\n",
    "if len(test_pairs) > 0:\n",
    "    print(\"\\nEvaluating generation quality...\\n\")\n",
    "\n",
    "    base_metrics = evaluate_generation_quality(base_model, base_tokenizer, test_pairs)\n",
    "    ft_metrics = evaluate_generation_quality(ft_model, ft_tokenizer, test_pairs)\n",
    "\n",
    "    print(\"Base Model:\")\n",
    "    print(f\"  ROUGE-L: {base_metrics['rouge_l']:.4f}\")\n",
    "    print(f\"  BLEU: {base_metrics['bleu']:.4f}\")\n",
    "\n",
    "    print(\"\\nFine-tuned Model:\")\n",
    "    print(f\"  ROUGE-L: {ft_metrics['rouge_l']:.4f}\")\n",
    "    print(f\"  BLEU: {ft_metrics['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777f579",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def compare_responses(prompt, base_model, base_tokenizer, ft_model, ft_tokenizer):\n",
    "    \"\"\"\n",
    "    Generate and compare responses from both models\n",
    "    \"\"\"\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Base model\n",
    "    inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs, max_new_tokens=150, temperature=0.7, do_sample=True\n",
    "        )\n",
    "    base_response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)[\n",
    "        len(prompt) :\n",
    "    ]\n",
    "\n",
    "    print(\"\\nBase Model Response:\")\n",
    "    print(base_response.strip())\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "    # Fine-tuned model\n",
    "    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(ft_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = ft_model.generate(\n",
    "            **inputs, max_new_tokens=150, temperature=0.7, do_sample=True\n",
    "        )\n",
    "    ft_response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)[\n",
    "        len(prompt) :\n",
    "    ]\n",
    "\n",
    "    print(\"\\nFine-tuned Model Response:\")\n",
    "    print(ft_response.strip())\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "comparison_prompts = [\n",
    "    \"What is the Communiverse?\",\n",
    "    \"Tell me about Elio's journey.\",\n",
    "    \"Who is Glordon and what role does he play?\",\n",
    "]\n",
    "\n",
    "print(\"\\nSide-by-Side Comparison:\\n\")\n",
    "for prompt in comparison_prompts:\n",
    "    compare_responses(prompt, base_model, base_tokenizer, ft_model, ft_tokenizer)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbc0b7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Compile all results\n",
    "evaluation_results = {\n",
    "    \"model_info\": {\"base_model\": BASE_MODEL, \"finetuned_model\": FINETUNED_MODEL},\n",
    "    \"perplexity\": {\n",
    "        \"base\": base_ppl,\n",
    "        \"finetuned\": ft_ppl,\n",
    "        \"improvement_percent\": improvement,\n",
    "    },\n",
    "    \"generation_quality\": {\n",
    "        \"base\": base_metrics if \"base_metrics\" in locals() else None,\n",
    "        \"finetuned\": ft_metrics if \"ft_metrics\" in locals() else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "import json\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {RESULTS_DIR}/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c3e8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af145f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create comparison chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perplexity comparison\n",
    "models = [\"Base Model\", \"Fine-tuned\"]\n",
    "ppls = [base_ppl, ft_ppl]\n",
    "axes[0].bar(models, ppls, color=[\"#ff7f0e\", \"#2ca02c\"])\n",
    "axes[0].set_ylabel(\"Perplexity (lower is better)\")\n",
    "axes[0].set_title(\"Perplexity Comparison\")\n",
    "axes[0].set_ylim(0, max(ppls) * 1.2)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(ppls):\n",
    "    axes[0].text(i, v + max(ppls) * 0.02, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "# Generation quality comparison (if available)\n",
    "if \"base_metrics\" in locals():\n",
    "    metrics = [\"ROUGE-L\", \"BLEU\"]\n",
    "    base_vals = [base_metrics[\"rouge_l\"], base_metrics[\"bleu\"]]\n",
    "    ft_vals = [ft_metrics[\"rouge_l\"], ft_metrics[\"bleu\"]]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[1].bar(x - width / 2, base_vals, width, label=\"Base Model\", color=\"#ff7f0e\")\n",
    "    axes[1].bar(x + width / 2, ft_vals, width, label=\"Fine-tuned\", color=\"#2ca02c\")\n",
    "\n",
    "    axes[1].set_ylabel(\"Score (higher is better)\")\n",
    "    axes[1].set_title(\"Generation Quality Metrics\")\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(metrics)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/evaluation_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to: {RESULTS_DIR}/evaluation_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
